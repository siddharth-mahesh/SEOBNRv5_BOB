{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLLFfoXq_pW9"
   },
   "source": [
    "Written by Anuj Kankani and Siddarth Mahesh                                     \n",
    "Any bugs are Matt Cerep's fault\n",
    "\n",
    "A large chunk of the SXS catalog is stored on Anuj's thornyflat scratch account                                                                         \n",
    "If you need access ask Anuj and he can add you to the allowed users list        \n",
    "This way you don't have to redownload the catalog, and any files you download from sxs can be used by everyone                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3i3bXlyUyy-X",
    "outputId": "8454c161-74f5-4075-e6f4-948fa054a899"
   },
   "outputs": [],
   "source": [
    "#!pip install sxs\n",
    "#!pip install kuibit\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-5LSlzXxyZWm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sxs\n",
    "import kuibit\n",
    "from kuibit.gw_mismatch import mismatch_from_strains\n",
    "from kuibit.timeseries import TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wJJqFcRz6Es"
   },
   "source": [
    "This just sets the necessary config options for downloading, and lets you see where the data is being downloaded. If you want to change the data storage location on your machine you have to change the environment variables SXSCONFIGDIR and SXSCACHEDIR. Note: On the cluster make sure you do this, otherwise it will NOT download to scratch by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFiGS2iJz1pk",
    "outputId": "7aea0228-57b2-4218-b79d-cb79f89047a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome!\n",
      "config directory is /home/siddharth-mahesh/.config/sxs\n",
      "cache directory is /home/siddharth-mahesh/.cache/sxs\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome!\")\n",
    "sxs.write_config(download=True,cache=True)\n",
    "print(\"config directory is\",sxs.sxs_directory(\"config\"))\n",
    "print(\"cache directory is\",sxs.sxs_directory(\"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrmaPqnz0fig"
   },
   "source": [
    "Get the catalog information from SXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9Y3xEHAt0dyM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download from 'https://data.black-holes.org/catalog.json' because local file is newer\n"
     ]
    }
   ],
   "source": [
    "catalog = sxs.load(\"catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efmkVr3-0vox"
   },
   "source": [
    "This determines the simulations we will use. This code block can obviosuly be edited as needed to get the simulations we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d_soTZPv0qbb"
   },
   "outputs": [],
   "source": [
    "# There is probably a shorter way to do this, but I'm not familiar enough with dataframes, and this is more user friendly imo.\n",
    "# SXS defines reference values which are calculated after the junk radiation has settled down.\n",
    "def get_sims(catalog):\n",
    "  df = catalog.table #create a Pandas dataframe\n",
    "  BBH_sims = df.loc[df['object_types'] == 'BHBH']\n",
    "  r,c = BBH_sims.shape\n",
    "  print(\"There are\",r,\"total BBH simulations in the catalog\")\n",
    "\n",
    "  #print(df.columns) #uncomment this if you want to see all the columns in the catalog\n",
    "\n",
    "  SPIN_TOLERANCE = 1e-5\n",
    "  MAXIMUM_ECCENTRICITY = 1e-3\n",
    "\n",
    "  index_arr = []\n",
    "\n",
    "  for index, row in BBH_sims.iterrows():\n",
    "      chi_x1 = row['reference_dimensionless_spin1'][0]\n",
    "      chi_y1 = row['reference_dimensionless_spin1'][1]\n",
    "      chi_z1 = row['reference_dimensionless_spin1'][2]\n",
    "      chi_x2 = row['reference_dimensionless_spin2'][0]\n",
    "      chi_y2 = row['reference_dimensionless_spin2'][1]\n",
    "      chi_z2 = row['reference_dimensionless_spin2'][2]\n",
    "      ecc = row['reference_eccentricity']\n",
    "      mass_ratio = row['reference_mass_ratio']\n",
    "\n",
    "      #make sure x and y components of spin are under tolerance as well as the eccentricity\n",
    "      if(abs(chi_x1)<=SPIN_TOLERANCE and abs(chi_y1)<=SPIN_TOLERANCE and abs(chi_x2)<=SPIN_TOLERANCE and abs(chi_y2)<=SPIN_TOLERANCE and abs(ecc)<MAXIMUM_ECCENTRICITY):\n",
    "        index_arr.append(index)\n",
    "\n",
    "  sims = BBH_sims[BBH_sims.index.isin(index_arr)]\n",
    "  r,c = sims.shape\n",
    "  print(\"Found\",r,\"BBH simulations matching conditions\")\n",
    "\n",
    "  return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiyL_b-O14X-"
   },
   "source": [
    "sims is now a Pandas dataframe with all the catalog information for the simulations we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aqjgv5Lg6WoA",
    "outputId": "73b30dd7-05e2-4ae6-aedd-8ceae8ff3312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2019 total BBH simulations in the catalog\n",
      "Found 436 BBH simulations matching conditions\n"
     ]
    }
   ],
   "source": [
    "sims = get_sims(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEdrbSIY3Eem"
   },
   "source": [
    "The search_expression variable defines the file that will be used in every simulation. This variable can be edited to get multiple resolution levels, psi4 data, etc... More information can be found at the bottom of https://sxs.readthedocs.io/en/main/tutorials/02-Catalog/ in the \"Selecting Data Sets\" section as well as in the source code. search_expression=\"/Lev/rhOverM\" searches for the latest version, highest resolution strain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QWT3FKyr8MgH"
   },
   "outputs": [],
   "source": [
    "search_expression = \"/Lev/rhOverM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtBdk5_X5RXl"
   },
   "source": [
    "First we can check the size of the file we will download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IaLTfm2z3BX_"
   },
   "outputs": [],
   "source": [
    "def get_file_size(catalog,sims,search_expression):\n",
    "  id = np.array(sims.index)\n",
    "  total_file_size = 0\n",
    "  for str in id:\n",
    "      #this assumes every sim is identified as SXS:BBH:####v# where # is a number\n",
    "      BBH_sim = catalog.select_files(str+search_expression)\n",
    "      for key,subdict in BBH_sim.items():\n",
    "          total_file_size += subdict['filesize']\n",
    "\n",
    "  print(\"total size is \",(total_file_size/1000000)/1000,' Gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3_Z8Dhj7Da6",
    "outputId": "c01a868d-ffb4-400b-db26-bd7a74dcecd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size is  43.272072269999995  Gb\n"
     ]
    }
   ],
   "source": [
    "get_file_size(catalog,sims,search_expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbILS8cq5mfL"
   },
   "source": [
    " We need to specify the extrapolation order we want to use. SXS:BBH:1111 only has a 2nd order extrapolation, but everything else has 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MXIfwzG-5pe5"
   },
   "outputs": [],
   "source": [
    "EXTRAPOLATION_ORDER = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2pyMNnh7MVb"
   },
   "source": [
    "Finally, we can download the waveforms into our cache directory. If the file already exists in our cache, it will not download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bD4slY0d5aS0"
   },
   "outputs": [],
   "source": [
    "def download_waveforms(sims,search_expression,EXTRAPOLATION_ORDER = 4):\n",
    "  for str in id:\n",
    "    try:\n",
    "      sxs.load(str+search_expression, extrapolation_order=EXTRAPOLATION_ORDER)\n",
    "    except:\n",
    "      print(\"Could not find given extrapolation order. Trying 2nd order extrapolation\")\n",
    "      sxs.load(str+search_expression, extrapolation_order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UVf23bNC7WoV"
   },
   "outputs": [],
   "source": [
    "#download_waveforms(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtcFl2UbWeTk"
   },
   "source": [
    "Everything below is analysis code. The general principle is to take a sxs waveform, convert the needed modes to kuibit timeseries and use kuibit's built in functionality as much as possible. Nothing has been tested here yet. All analysis functions that take in a sxs waveform assume that the junk radiation has been sliced off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "375lrRJQUYw1"
   },
   "source": [
    "Helper function to find nearest value in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "liT6e1MsUYN9"
   },
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sBvzL4UrhZDL"
   },
   "outputs": [],
   "source": [
    "def find_nearest_index(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrEjupRCNl5H"
   },
   "source": [
    "Quick helper function that takes in a sxs waveform and returns a kuibit timeseries for the given l,m mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zlGgbBOnNh27"
   },
   "outputs": [],
   "source": [
    "def get_kuibit_lm(w,l,m):\n",
    "    index = w.index(l, m)\n",
    "    w_temp = w[:,index]\n",
    "    return TimeSeries(w.t,w_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJFnWx0_OlrT"
   },
   "source": [
    "Compute the time difference between the peak strain of two modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9PCIwhkoOtjr"
   },
   "outputs": [],
   "source": [
    "def get_peak_strain_time_difference_between_modes(w,l1,m1,l2,m2):\n",
    "  t1 = get_kuibit_lm(w,l1,m1)\n",
    "  t2 = get_kuibit_lm(w,l2,m2)\n",
    "  return t1.time_at_maximum()-t2.time_at_maximum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejnD1FOVTFk0"
   },
   "source": [
    "return the waveform frequency as a timeseries for a given l,m mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6CwH1jbCSHMg"
   },
   "outputs": [],
   "source": [
    "def get_kuibit_frequency_lm(w,l,m):\n",
    "  ts = get_kuibit_lm(w,l,m)\n",
    "  #I'm pretty sure this is right, but if there is a mistake somewhere it's probably here\n",
    "  return ts.phase_angular_velocity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrvXTFA1U31R"
   },
   "source": [
    "return the isco frequency given a final mass and spin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TUAqR7LeU1-t"
   },
   "outputs": [],
   "source": [
    "def get_w_isco(m,mf,af):\n",
    "  # note that m is the multipole moment of the waveform\n",
    "  # get r_isco and Omega_isco first (See Bardeen, Press, Teukolsky 1972)\n",
    "  Z1 = 1. + ((1. - af*af)**(1./3.))*((1 + af)**(1/3) + (1 - af)**(1/3))\n",
    "  Z2 = np.sqrt(3*af*af + Z1*Z1)\n",
    "  r_isco = (3 + Z2 - np.sqrt(3 - Z1)*(3 + Z1 + 2*Z2))*mf\n",
    "  Omega_isco = 1/((r_isco**1.5 + af)*mf)\n",
    "  w_isco = m*Omega_isco\n",
    "  return w_isco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maVDqbURVRh6"
   },
   "source": [
    "return the time difference between the peak strain and t_isco for a given l,m mode, Assumes junk radiation has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sTS4ZxU3T98G"
   },
   "outputs": [],
   "source": [
    "def get_isco_peak_strain_time_difference_lm(w,l,m,mf,af):\n",
    "  ts = get_kuibit_lm(w,l,m)\n",
    "  freq = get_kuibit_frequency_lm(w,l,m)\n",
    "  w_isco = get_w_isco(m,mf,af)\n",
    "  t_peak = ts.time_at_maximum()\n",
    "  #There may be cases where the waveform well after the peak strain becomes noisy and reaches w_isco again, so we only look for w_isco before the peak strain\n",
    "  freq = freq.cropped(end=t_peak)\n",
    "  t_isco = freq.t[find_nearest_index(freq.y,w_isco)]\n",
    "  return t_peak-t_isco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dL1Kvc6s0Q4"
   },
   "source": [
    "Calculate the mismatch between a sxs waveform and a Fully(Freedom) Effective One Body model for a given l,m mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cVG7HVD9sSn7"
   },
   "outputs": [],
   "source": [
    "#NEEDS TO BE TESTED, ALSO SHOULD CHECK THAT KUIBIT CALCULATES MISMATCHES THE SAME WAY SEOBNR DOES\n",
    "#NEED TO ADD FUNCTIONALITY TO MAKE SURE START AND END TIMES ARE THE SAME\n",
    "#probably should just pass in a start and end time and use kuibit to crop the data? idk how you wrote the BOB code so not messing with this for now\n",
    "def calculate_mismatch(w,FEOB,SEOB,l,m):\n",
    "  sxs = get_kuibit_lm(w,l,m)\n",
    "  mismatch_FEOB = mismatch_from_strains(sxs,FEOB)\n",
    "  mismatch_SEOB = mismatch_from_strains(sxs,SEOB)\n",
    "  return mismatch_FEOB, mismatch_SEOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddsB8vDV91pH"
   },
   "source": [
    "Now that we have downloaded all our waveforms, we can load them and analyze as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6z_MIebJ9euH"
   },
   "outputs": [],
   "source": [
    "#you can convert all the modes into a kuibit GravitationalWavesOneDet, but kuibit assumes GravitationalWavesOneDet is psi4 at a finite distance\n",
    "#you can easily work around this by setting r = 1 for extrapolated data and just being aware your data is the strain\n",
    "#but in the interest of making this as easy to work with, without requiring too much kuibit knowledge, and avoiding future mistakes\n",
    "#the analysis functions are built to take in a sxs waveform with junk radiation removed, convert the necessary modes to kuibit timeseries and go from there\n",
    "def analyze_waveforms(sims,search_expression,EXTRAPOLATION_ORDER=4):\n",
    "  delta_t = []\n",
    "  x_arr = []\n",
    "  for index, row in sims.iterrows():\n",
    "    try:\n",
    "      w = sxs.load(index+search_expression, extrapolation_order=EXTRAPOLATION_ORDER)\n",
    "    except:\n",
    "      print(\"Could not find given extrapolation order. Trying 2nd order extrapolation\")\n",
    "      w = sxs.load(str+search_expression, extrapolation_order=2)\n",
    "    \n",
    "    relax_time = row['relaxation_time']\n",
    "    #this removes junk radiation according to the time calculated by sxs. sxs warns this is a very rough estimate\n",
    "    #The analysis functions assume junk radiation has been taken care of so it is necessary to remove junk radiation here\n",
    "    w_sliced = w[w.index_closest_to(relax_time):]\n",
    "    rem_mass = row['remnant_mass']\n",
    "    rem_spin = row['remnant_dimensionless_spin']\n",
    "    #as an example we can plot the time difference between the peak strain time of the (2,2) and (4,4) modes as a function of the initial mass ratio\n",
    "    delta_t.append(get_peak_strain_time_difference_between_modes(w_sliced,l1=2,m1=2,l2=4,m2=4))\n",
    "    x_arr.append(row['initial_mass_ratio'])\n",
    "  return x_arr, delta_t\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Dj6kBtN0gC4j"
   },
   "outputs": [],
   "source": [
    "#in case you want to see the possible column values you can call\n",
    "#print(sims.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EVf7qZz4X_k5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following files to load from the SXS catalog:\n",
      "    SXS:BBH:0001v6/Lev5/rhOverM_Asymptotic_GeometricUnits_CoM.h5\n",
      "Found the following files to load from the SXS catalog:\n",
      "    SXS:BBH:0002v7/Lev6/rhOverM_Asymptotic_GeometricUnits_CoM.h5\n",
      "Downloading to /home/siddharth-mahesh/.cache/sxs/SXS:BBH:0002v4/Lev6/rhOverM_Asymptotic_GeometricUnits_CoM.h5:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c594d16c11984e2d88732535629e7e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                     | 0/166211179 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following files to load from the SXS catalog:\n",
      "    SXS:BBH:0004v5/Lev6/rhOverM_Asymptotic_GeometricUnits_CoM.h5\n",
      "Downloading to /home/siddharth-mahesh/.cache/sxs/SXS:BBH:0004v2/Lev6/rhOverM_Asymptotic_GeometricUnits_CoM.h5:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b8d9596d374e8a8634bd41f7ceb5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                     | 0/158296692 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find given extrapolation order. Trying 2nd order extrapolation\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'type' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m, in \u001b[0;36manalyze_waveforms\u001b[0;34m(sims, search_expression, EXTRAPOLATION_ORDER)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m   w \u001b[38;5;241m=\u001b[39m sxs\u001b[38;5;241m.\u001b[39mload(index\u001b[38;5;241m+\u001b[39msearch_expression, extrapolation_order\u001b[38;5;241m=\u001b[39mEXTRAPOLATION_ORDER)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sxs/handlers.py:230\u001b[0m, in \u001b[0;36mload\u001b[0;34m(location, download, cache, progress, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     download_url \u001b[38;5;241m=\u001b[39m file_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 230\u001b[0m     download_file(download_url, path, progress\u001b[38;5;241m=\u001b[39mprogress)\n\u001b[1;32m    231\u001b[0m paths\u001b[38;5;241m.\u001b[39mappend(path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sxs/utilities/downloads.py:91\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, path, progress, if_newer)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mwrapattr(r\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mfile_size, desc\u001b[38;5;241m=\u001b[39mdesc, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m r_raw:\n\u001b[0;32m---> 91\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(r_raw, f)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/shutil.py:197\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     buf \u001b[38;5;241m=\u001b[39m fsrc_read(length)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/utils.py:175\u001b[0m, in \u001b[0;36mCallbackIOWrapper.__init__.<locals>.read\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 175\u001b[0m     data \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    176\u001b[0m     callback(\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_arr, delta_t \u001b[38;5;241m=\u001b[39m analyze_waveforms(sims,search_expression,\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(x_arr,delta_t)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m, in \u001b[0;36manalyze_waveforms\u001b[0;34m(sims, search_expression, EXTRAPOLATION_ORDER)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find given extrapolation order. Trying 2nd order extrapolation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m   w \u001b[38;5;241m=\u001b[39m sxs\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m+\u001b[39msearch_expression, extrapolation_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m relax_time \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelaxation_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#this removes junk radiation according to the time calculated by sxs. sxs warns this is a very rough estimate\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#The analysis functions assume junk radiation has been taken care of so it is necessary to remove junk radiation here\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'type' and 'str'"
     ]
    }
   ],
   "source": [
    "x_arr, delta_t = analyze_waveforms(sims,search_expression,4)\n",
    "plt.scatter(x_arr,delta_t)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
